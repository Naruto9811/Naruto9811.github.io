---
title: "Word2vec Parameter Learning Explained"
author_profile: true
date: 2019-10-24
tags: [Natural Language Processing, word2vec]
mathjax: "true"
header:
    image: "/imgs/blog8.jpg"
excerpt: "Natural Language Processing, word2vec"
---

## 本文探讨的主题：word2vec的理解

## 本文想要解决的问题：从直观的角度理清Mikolov文章中的思路

在研究Mikolov的word2vec两篇原始论文之前，遇到了比较大的理解困难。于是决定先进行对原始论文进行解读的相关论文开始看起。由此选择此篇于网上评价极高的解释性论文进行学习。兹此予以记录。

## Word2vec

NLP从处理细粒度的角度来看，word2vec处理的事最为细腻的词语级别语言。

NLP中的词语是符号形式的，通过词嵌入的方法转换成数值形式的才能使得**神经网络，SVM等只接受数值型输入的模型结构**得到输入。

把NLP的语言处理模型抽象化为：

$$f(x) \rightarrow y$$

把 x 看作句子中的一个词语，把 y 看作词语的上下文，那么 f 就是一个语言模型，模型作用就是判断**（x,y）这个样本是否是人话，说不说地通**

**Word2vec**就是这样一个语言模型，但它的最终目的不是把 f 训练地非常完美，而是关心模型训练后得到的模型参数，将这些参数作为 输入 的向量化表示（词向量）

**Word2vec**的模型的优越性在于它在变换的时候只进行了线性变换。在隐藏层并没有进行非线性变换，这使得word2vec模型显得较为优越。

## Skip-gram和CBOW模型

* 把一个词语作为输入，预测它周围的上下文，这个模型叫做【Skip-gram】
* 把一个词语的上下文作为输入，预测这个词语本身，则是【CBOW】

## CBOW模型的 one-word context 形式

当 X 输入是一个词语，Y 输入也是一个词语的时候可以认为是两个模型的一种特殊形式（**模型任务特殊化为用当前词X去预测下一个词Y**）



#### simple CBOW模型结构

1）在此simple CBOW模型中，由三层构成 input -- hidden -- output。三层间是完全连接的。

2）input层使用的是one-hot编码向量