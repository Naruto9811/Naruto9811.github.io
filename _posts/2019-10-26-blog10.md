---

title: "Efficient Estimation of Word Representations in Vector Space"
author_profile: true
date: 2019-10-26
tags: [Natural Language Processing, word2vec]
mathjax: "true"
header:
    image: "/imgs/blog10.png"
excerpt: "Natural Language Processing, word2vec"
---

## 本文探讨的主题：word2vec原文

## 本文想要解决的问题：证明word2vec的优越性和跨时代意义

此篇是Mikolov的原文。比较简略地阐述了word2vec的模型，但是比较完整地交代了word2vec 与其他模型之间的关系并且清楚地证明了word2vec 模型的优越性。

## word2vec的意义

提出了CBOW和Skip-Gram 两种在很大的数据集上计算连续的词向量的方法。

这些词向量的质量通过word similarity task来进行比较。

（**比较语法和语义相似性，其中语法相似性指big 和 bigger ，apparent和apparently，而语义相似性指Athens 和 Greece，brother和sister**）

实验发现在更小的计算代价下word2vec相比其他模型获得了极大的进步。

word2vec的优越点：

1. word2vec是当时唯一一个能在几十亿个单词的训练集上在几百万个单词组成的语料库训练几百万个单词的词向量的模型，并且词向量维度保持在50-100。
2. word2vec模型训练出的词向量拥有多重相似性。名词结尾**相似**或者相同的单词有相似词向量，在名词周围能找到结尾相似的单词【语法方面】
3. 语义相同的词会有相似词向量，king – queen = man - woman在word2vec下的词向量中成立【语义方面】
4. 通过新模型的发明，保留了单词间的线性规律性，证明了单词的线性规律性能够在word2vec中很准确地学习到
5. hidden layer中没有非线性激活函数

## 判断word embedding 良好与否的判断标准

当前绝大部分工作（比如以各种方式改进word embedding）都是依赖wordsim353等词汇相似性数据集进行相关性度量，并以之作为评价word embedding质量的标准。然而，这种基于similarity的评价方式对训练数据大小、领域、来源以及词表的选择非常敏感。而且数据集太小，往往并不能充分说明问题。

而由于NLP是AI中任务导向性非常明确的一个学科分支。

所以应该以word embedding对于实际任务的收益为评价标准。包括词汇类比任务（所谓的analogy task，如king – queen = man - woman）以及NLP中常见的应用任务，比如命名实体识别（NER），句法分析（parsing）等。

把word embedding的评价标准下放到具体任务，才更有比较embedding 好不好的意义。如果脱离了实际意义，那么就很难将谁的word embedding好，谁的word embedding 差。

在word2vec的原文中，Mikolov定义了一个非常全面的测试集，包括5种语义问题，9种语法问题。从一个比较全面的角度证明了：

**word2vec是对于NLP中语法和语义任务都是有普遍价值的，是一种非常有效且有普适性的一种word embedding方法。**





